{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "csv_file_path = 'news_stock_price.csv'\n",
    "\n",
    "with open(csv_file_path, newline='') as csvfile:\n",
    "    myreader = csv.reader(csvfile)\n",
    "\n",
    "    next(myreader)\n",
    "    headers = next(myreader)\n",
    "    print(\"Column names:\", headers)\n",
    "\n",
    "    print(headers[1])\n",
    "    row_count = 0\n",
    "    for row in myreader:\n",
    "        if row_count == 50:\n",
    "            break\n",
    "        if row:\n",
    "            for column in row:\n",
    "                print(column)\n",
    "\n",
    "            print()\n",
    "        row_count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOKENIZING AND NORMALIZNG THE FULL TEXT COLUMN\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import bigrams\n",
    "from collections import Counter\n",
    "\n",
    "csv_file_path = 'news_stock_price.csv'\n",
    "\n",
    "with open(csv_file_path, newline='') as csvfile:\n",
    "    myreader = csv.reader(csvfile)\n",
    "    next(myreader)\n",
    "    \n",
    "    ft_rows = []\n",
    "    for row in myreader:\n",
    "        ft_rows.append(row[2])\n",
    "\n",
    "    row_count = 0\n",
    "    for text in ft_rows:\n",
    "        if row_count < 50:\n",
    "            row_count += 1\n",
    "            \n",
    "    def custom_tokenize(text):\n",
    "        protected_text = text.replace('S&P 500', 'SNP500')\n",
    "        tokens = nltk.word_tokenize(protected_text)\n",
    "        restored_tokens = []\n",
    "        for token in tokens:\n",
    "            if token == 'SNP500':\n",
    "                restored_tokens.extend(['S&P', '500'])\n",
    "            else:\n",
    "                restored_tokens.append(token)\n",
    "        \n",
    "        return restored_tokens\n",
    "\n",
    "    combined_text = ' '.join(ft_rows)\n",
    "\n",
    "    stop_words = stopwords.words('english')\n",
    "    stop_words.extend([\"said\", \"also\", \"like\"])\n",
    "    \n",
    "    tokens = custom_tokenize(combined_text)\n",
    "    \n",
    "    normalized_tokens = [word for word in tokens if (word.isalpha() and word.lower() not in stop_words)or word in ['S&P', '500']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FINDING OUT STATISTICS ABOUT OUR DATA\n",
    "import csv\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "total_sentences = 0\n",
    "total_words = 0\n",
    "total_tokens = 0\n",
    "unique_tokens = set()\n",
    "unique_words = set()\n",
    "\n",
    "for text in ft_rows:\n",
    "    sentences = sent_tokenize(text)\n",
    "    total_sentences += len(sentences)\n",
    "\n",
    "    report_tokens = custom_tokenize(text)\n",
    "    total_tokens += len(report_tokens)\n",
    "    total_words += len([token for token in report_tokens if token.isalpha()])\n",
    "\n",
    "    unique_tokens.update(report_tokens)\n",
    "    unique_words.update([token.lower() for token in report_tokens if token.isalpha()])\n",
    "\n",
    "average_sentences = total_sentences / len(ft_rows) if ft_rows else 0\n",
    "average_words = total_words / len(ft_rows) if ft_rows else 0\n",
    "average_tokens = total_tokens / len(ft_rows) if ft_rows else 0\n",
    "\n",
    "num_unique_tokens = len(unique_tokens)\n",
    "num_unique_words = len(unique_words)\n",
    "\n",
    "print(\"Average number of sentences per report:\", average_sentences)\n",
    "print(\"Average number of words per report:\", average_words)\n",
    "print(\"Average number of tokens per report:\", average_tokens)\n",
    "print(\"Total number of unique words:\", num_unique_words)\n",
    "print(\"Total number of unique tokens:\", num_unique_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PRINTS OUT THE FIRST 500 tokens in the tokenized and normalized list\n",
    "tcount = 0\n",
    "tstring = ''\n",
    "for token in normalized_tokens:\n",
    "    if tcount < 500:\n",
    "        tstring += token + ' '\n",
    "        tcount += 1\n",
    "print(tstring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PRINTS THE TOP 50 WORDS OF NEW TOKENIZED AND NORMALIZED LIST\n",
    "word_freq = nltk.FreqDist(normalized_tokens)\n",
    "top_50_words = word_freq.most_common(50)\n",
    "\n",
    "for word, freq in top_50_words:\n",
    "    print(f\"{word}: {freq}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import bigrams\n",
    "\n",
    "bigram_list = list(bigrams(normalized_tokens))\n",
    "\n",
    "bigram_counts = Counter(bigram_list)\n",
    "top_10_bigrams = bigram_counts.most_common(10)\n",
    "\n",
    "print(\"Top 10 Bigrams:\")\n",
    "for bigram, count in top_10_bigrams:\n",
    "    print(f\"{bigram}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAKING A WORD CLOUD FOR ALL TOKENS IN FULL_TEXT\n",
    "#IM NOT SURE IF WE SHOULD LEMMATIZE OR NOT, EITHER WAY S&P doesnt show up in cloud\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "all_lemmas = [lemmatizer.lemmatize(w) for w in normalized_tokens]\n",
    "\n",
    "text = \" \".join(all_lemmas)\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "word_freq = nltk.FreqDist(text)\n",
    "\n",
    "top_words = dict(word_freq.most_common(100))\n",
    "wordcloud = WordCloud(background_color='white').generate_from_frequencies(top_words)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(wordcloud, interpolation = 'bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1147\n"
     ]
    }
   ],
   "source": [
    "#FILTERS THE DATASET SO ONLY FULL TEXT REPORTS \n",
    "#WHERE A SPECIFIC WORD IS MENTIONED IN IT ARE INCLUDED. IT THEN ADDS THOSE REPORTS\n",
    "#TO A NEW LIST(apple_rows) OF REPORTS THAT ALL INCLUDE THAT WORD \n",
    "#WE CAN USE THIS TO FIND BIGRAMS, DISTRIBUTIONS, and WORD CLOUDS FOR ALL REPORTS\n",
    "#THAT MENTION THAT COMPANY\n",
    "\n",
    "import csv\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import bigrams\n",
    "from collections import Counter\n",
    "import string\n",
    "import spacy\n",
    "\n",
    "csv_file_path = 'news_stock_price.csv'\n",
    "\n",
    "column_name_to_search = 'full_text'\n",
    "\n",
    "#SPECIFIC WORD TO MENTION. We can use this to find reports that mention a specific company\n",
    "word_to_search = 'Tesla'\n",
    "\n",
    "apple_rows = []\n",
    "\n",
    "with open(csv_file_path, newline='') as csvfile:\n",
    "    myreader = csv.reader(csvfile)\n",
    "\n",
    "    next(myreader)\n",
    "    headers = next(myreader)\n",
    "\n",
    "    column_index = None\n",
    "\n",
    "    column_index = headers.index(column_name_to_search)\n",
    "    \n",
    "    if column_index is not None:\n",
    "        for row in myreader:\n",
    "            if len(row) > column_index:\n",
    "                if word_to_search in row[column_index]:\n",
    "                    apple_rows.append(row[2])\n",
    "apple_count = 0\n",
    "for text in apple_rows:\n",
    "    apple_count += text.count(word_to_search)\n",
    "print(apple_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOKENIZES AND NORMALIZES THE TEXT THAT MENTIONS THE COMPANY IN THE CELL ABOVE\n",
    "#PRETTY MUCH THE SAME CODE AS I TOKENIZED BEFORE\n",
    "#WE CAN ADD MORE WORDS/PHRASES TO CUSTOM TOKENIZE IF WE WANT TO PROTECT THEM (MAYBE % SIGN)\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import bigrams\n",
    "from collections import Counter\n",
    "\n",
    "def custom_tokenize(text):\n",
    "    protected_text = text.replace('S&P 500', 'SNP500')\n",
    "    tokens = nltk.word_tokenize(protected_text)\n",
    "\n",
    "    restored_tokens = []\n",
    "    for token in tokens:\n",
    "        if token == 'SNP500':\n",
    "            restored_tokens.extend(['S&P', '500'])\n",
    "        else:\n",
    "            restored_tokens.append(token)\n",
    "    \n",
    "    return restored_tokens\n",
    "\n",
    "combined_text = ' '.join(apple_rows)\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend([\"said\", \"also\", \"like\"])\n",
    "\n",
    "tokens = custom_tokenize(combined_text)\n",
    "normalized_tokens = [word for word in tokens if (word.isalpha() and word.lower() not in stop_words) or word in ['S&P', '500']]\n",
    "bigram_list = list(bigrams(normalized_tokens))\n",
    "\n",
    "bigram_counts = Counter(bigram_list)\n",
    "top_10_bigrams = bigram_counts.most_common(10)\n",
    "\n",
    "print(\"Top 10 Bigrams:\")\n",
    "for bigram, count in top_10_bigrams:\n",
    "    print(f\"{bigram}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FINDING COLLOCATIONS\n",
    "from nltk.collocations import *\n",
    "\n",
    "def find_collocations(tokens, num):\n",
    "    bigram_measures = BigramAssocMeasures()\n",
    "    finder = BigramCollocationFinder.from_words(normalized_tokens)\n",
    "\n",
    "    finder.apply_freq_filter(10)\n",
    "    top_n_bigrams = finder.nbest(bigram_measures.pmi, num)\n",
    "    bigram_freqs = [(bigram, finder.ngram_fd[bigram]) for bigram in top_n_bigrams]\n",
    "    return bigram_freqs\n",
    "apple_collocations = find_collocations(normalized_tokens, 10)\n",
    "\n",
    "print(\"Top 10 Collocations:\")\n",
    "for bigram, count in apple_collocations:\n",
    "    print(f\"{bigram}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PRINTS FIRST 500 TOKENS AS STRING\n",
    "newstring = ''\n",
    "tcount = 0\n",
    "for token in normalized_tokens:\n",
    "    if tcount < 10000:\n",
    "        newstring += token + ' '\n",
    "        tcount +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv  \n",
    "from collections import defaultdict, Counter\n",
    "import pandas as pd  \n",
    "from datetime import datetime  # For handling date and time\n",
    "import matplotlib.pyplot as plt  \n",
    "\n",
    "# Defining a list of target words to track\n",
    "target_words = [\"Apple\", \"Facebook\", \"Microsoft\", \"Tesla\", \"Amazon\"]\n",
    "\n",
    "# Creating a defaultdict to store word frequencies over time\n",
    "word_frequency_over_time = defaultdict(Counter)\n",
    "\n",
    "with open('news_stock_price.csv', newline='', encoding='utf-8') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader)  # Skipping the header row\n",
    "    for row in reader:\n",
    "        # Extracting news text and date from the row\n",
    "        news_text = row[2]\n",
    "        date = datetime.strptime(row[0], '%Y-%m-%d')  # Converting date string to datetime object\n",
    "\n",
    "        # Splitting the news text into tokens (words)\n",
    "        tokens = news_text.split()\n",
    "        # Counting occurrences of target words in the news text\n",
    "        word_counts = Counter(word for word in tokens if word in target_words)\n",
    "        \n",
    "        word_frequency_over_time[date].update(word_counts)\n",
    "\n",
    "# Converting the defaultdict to a pandas DataFrame\n",
    "df = pd.DataFrame.from_dict(word_frequency_over_time, orient='index')\n",
    "# Filling missing values with 0\n",
    "df = df.fillna(0)\n",
    "\n",
    "# Plotting the data\n",
    "df.plot(figsize=(12, 6))\n",
    "plt.xlabel('Date')  \n",
    "plt.ylabel('Word Frequency')  \n",
    "plt.title('Word Frequency \n",
    "plt.legend(target_words)  \n",
    "plt.xticks(rotation=45)  \n",
    "plt.grid(True) \n",
    "\n",
    "# Saving the plot as an image file\n",
    "plt.savefig('word_frequency_trends.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import spacy  # For recognizing entities using \"en_core_web_lg\" model from spaCy\n",
    "import csv  \n",
    "from collections import Counter  # For counting entity occurrences\n",
    "\n",
    "# Load the large English model from Spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Function to extract specified entities from text\n",
    "def extract_entities(text):\n",
    "    # Processing the text with the Spacy NLP model\n",
    "    doc = nlp(text)\n",
    "    # Returning entities that are organizations, geopolitical entities, or products\n",
    "    return [ent.text for ent in doc.ents if ent.label_ in [\"ORG\", \"GPE\", \"PRODUCT\"]]\n",
    "\n",
    "# Initializing a counter to keep track of entity occurrences\n",
    "entity_counter = Counter()\n",
    "\n",
    "# Open and read the CSV file\n",
    "with open('news_stock_price.csv', newline='', encoding='utf-8') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader)  \n",
    "    # Iterating over each row in the CSV file\n",
    "    for row in reader:\n",
    "        news_text = row[2]\n",
    "        # Extracting entities from the news text\n",
    "        entities = extract_entities(news_text)\n",
    "        # Updating the counter with these entities\n",
    "        entity_counter.update(entities)\n",
    "\n",
    "# Printing the 10 most common entities\n",
    "print(\"Most common entities:\")\n",
    "for entity, count in entity_counter.most_common(10):\n",
    "    print(f\"{entity}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#draw plot using the ouput of last block, there is only eight since we combined \"US and U.S., \"NASDAQ and Nasdaq\"\n",
    "entities = [\"U.S.\", \"Fed\", \"Nasdaq\",\"Russia\", \"NYSE\", \"Ukraine\", \"China\", \"Treasury\"]\n",
    "frequencies = [5556 + 2520, 5403, 1495+1221, 2126, 2022, 1877, 1664, 1234]\n",
    "\n",
    "plt.figure(figsize=(10, 6))  # Adjust the figure size as needed\n",
    "plt.bar(entities, frequencies, color='blue')\n",
    "plt.xlabel('Entities')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Most Common Entities Histogram (Combined U.S. and US)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
