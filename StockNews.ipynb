{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1ebde52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CELL 1\n",
    "#TOKENIZING AND NORMALIZNG THE FULL TEXT COLUMN\n",
    "import csv\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import bigrams\n",
    "from collections import Counter\n",
    "\n",
    "csv_file_path = 'news_stock_price.csv'\n",
    "\n",
    "with open(csv_file_path, newline='') as csvfile:\n",
    "    #Reading the file in\n",
    "    myreader = csv.reader(csvfile)\n",
    "    #Must put next command because for our csv file our headers were on the second row\n",
    "    next(myreader)\n",
    "\n",
    "    #Create an empty list for all of column 2(which was the full text column)\n",
    "    ft_rows = []\n",
    "    for row in myreader:\n",
    "        ft_rows.append(row[2])\n",
    "\n",
    "    row_count = 0\n",
    "    for text in ft_rows:\n",
    "        if row_count < 50:\n",
    "            row_count += 1\n",
    "    #Custom tokenization method to prevent valuable bigrams like S&P 500 from being removed\n",
    "    def custom_tokenize(text):\n",
    "        protected_text = text.replace('S&P 500', 'SNP500')\n",
    "        tokens = nltk.word_tokenize(protected_text)\n",
    "        restored_tokens = []\n",
    "        for token in tokens:\n",
    "            if token == 'SNP500':\n",
    "                restored_tokens.extend(['S&P', '500'])\n",
    "            else:\n",
    "                restored_tokens.append(token)\n",
    "        \n",
    "        return restored_tokens\n",
    "\n",
    "    combined_text = ' '.join(ft_rows)\n",
    "    #creating stop words\n",
    "    stop_words = stopwords.words('english')\n",
    "    stop_words.extend([\"said\", \"also\", \"like\"])\n",
    "    \n",
    "    tokens = custom_tokenize(combined_text)\n",
    "    \n",
    "    normalized_tokens = [word for word in tokens if (word.isalpha() and word.lower() not in stop_words)or word in ['S&P', '500']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae88974a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in full text column:  3468111\n",
      "Total number of tokens in full text column:  4055377\n",
      "Total number of reports:  4700\n",
      "Average number of sentences per report: 33.72063829787234\n",
      "Average number of words per report: 737.8959574468086\n",
      "Average number of tokens per report: 862.846170212766\n"
     ]
    }
   ],
   "source": [
    "#CELL 2\n",
    "#FINDING OUT STATISTICS ABOUT OUR DATA\n",
    "import csv\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "total_sentences = 0\n",
    "total_words = 0\n",
    "total_tokens = 0\n",
    "unique_tokens = set()\n",
    "unique_words = set()\n",
    "count = 0\n",
    "for text in ft_rows:\n",
    "    #Total words\n",
    "    total_words += len(text.split())\n",
    "    #Total sentences using sentence tokenization\n",
    "    sentences = sent_tokenize(text)\n",
    "    total_sentences += len(sentences)\n",
    "    \n",
    "    #Total tokens\n",
    "    mytokens = nltk.word_tokenize(text)\n",
    "    total_tokens += len(mytokens)\n",
    " \n",
    "average_sentences = total_sentences / len(ft_rows)\n",
    "average_words = total_words / len(ft_rows)\n",
    "average_tokens = total_tokens / len(ft_rows)\n",
    "num_reports = len(ft_rows)\n",
    "\n",
    "print(\"Total number of words in full text column: \", total_words)\n",
    "print(\"Total number of tokens in full text column: \", total_tokens)\n",
    "print(\"Total number of reports: \", num_reports)\n",
    "print(\"Average number of sentences per report:\", average_sentences)\n",
    "print(\"Average number of words per report:\", average_words)\n",
    "print(\"Average number of tokens per report:\", average_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "853947a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read account statements third quarter anything answer probably already know stocks bonds year date Looking losses make smaller might make us feel smaller natural avoid looking closely evidence might undermine belief skilled investors markets however making good decisions often requires admitting things would much rather ignore Let start recognizing inertia choice Since end March stocks within whisker highs investors withdrawn billion stock mutual funds funds according Investment Company Institute international stock funds held trillion end March even though inflation seem fall stocks seem rise investors taken money stock funds partly sheer inertia partly millions people invest autopilot partly changing course losing money feels intensely painful every investor recognizes wisdom old saying Cut losses let profits run thing worse losing admit loser investors avoid selling investment pretend paper loss work later hand realize loss without realizing made mistake Worse sold could go back whatever put money instead could go feel idiot twice wonder selling loss hard many people freeze face bear market recent study researchers looked nearly traders international online brokerage used orders instructions designed limit much lose automatically selling investment falls predetermined price might buy stock set order theory handcuffing loss practice however people tear cuffs stock falls toward might drop stop loss say keeps dropping approaches cut stop loss maybe common kind behavior recent study time online traders already placed orders took action related positions lower thresholds even prices holdings fell farther traders dropped levels would automatically forced sell Instead stopping losses traders ended chasing intended quit Surely professional investors better selling Surely jest New research shows fund managers lose average percentage points return annually poor selling decisions managers tend sell either extreme recent winners average go outperform funds dump could done substantially better throwing dart portfolio selling whatever hit instead stocks actually sold says Alex Imas one study authors finance professor University Chicago learn whether selling decisions good track investments hold sold sold outperforming hold selling wrong never learn unless willing look Making peace losses requires planning ahead says Annie Duke cognitive psychologist former poker champion author book Quit Power Knowing Walk Away bias quitting really strong says facts conflict feelings find way ignore facts One best ways determine whether quit design advance Duke calls kill criteria commits set conditions investment else sold Let say bought bitcoin last year believed protective hedge inflation Putting kill criteria place would committed something along lines bitcoin goes inflation goes thesis disproved therefore must sell lose least period inflation exceeds people might different reasons owning bitcoin switch kill criteria fact rationale turned wrong would sell thereby avoiding much bitcoin nearly drop year investors buying holding usually right decision getting rid losers make one Disclaimer story published wire agency feed without modifications text Wall Street fell sharply Friday following solid jobs report September increased likelihood Federal Reserve barrel ahead interest rate hiking campaign many investors fear push economy recession Labor Department reported unemployment rate fell lower expectations economy continues show resilience despite Fed efforts bring high inflation weakening growth Nonfarm payrolls rose jobs \n"
     ]
    }
   ],
   "source": [
    "#CELL 3\n",
    "#PRINTS OUT THE FIRST 500 tokens in the tokenized and normalized list\n",
    "tcount = 0\n",
    "tstring = ''\n",
    "for token in normalized_tokens:\n",
    "    if tcount < 500:\n",
    "        tstring += token + ' '\n",
    "        tcount += 1\n",
    "print(tstring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c46b24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "market: 24298\n",
      "stock: 15747\n",
      "stocks: 12283\n",
      "investors: 10024\n",
      "year: 9671\n",
      "company: 7563\n",
      "inflation: 7236\n",
      "markets: 7033\n",
      "500: 6966\n",
      "S&P: 6532\n",
      "per: 6218\n",
      "growth: 6146\n",
      "Fed: 5702\n",
      "could: 5458\n",
      "trading: 5449\n",
      "would: 5427\n",
      "time: 5309\n",
      "companies: 5287\n",
      "prices: 5195\n",
      "week: 4924\n",
      "last: 4845\n",
      "earnings: 4794\n",
      "since: 4756\n",
      "higher: 4711\n",
      "one: 4678\n",
      "shares: 4672\n",
      "new: 4638\n",
      "rates: 4618\n",
      "rate: 4586\n",
      "years: 4581\n",
      "points: 4565\n",
      "billion: 4403\n",
      "index: 4341\n",
      "interest: 4332\n",
      "price: 4321\n",
      "percent: 4228\n",
      "high: 4105\n",
      "may: 3952\n",
      "Nasdaq: 3647\n",
      "data: 3564\n",
      "first: 3525\n",
      "economy: 3502\n",
      "investment: 3333\n",
      "share: 3296\n",
      "even: 3253\n",
      "quarter: 3236\n",
      "well: 3171\n",
      "financial: 3117\n",
      "Stock: 3105\n",
      "US: 3104\n"
     ]
    }
   ],
   "source": [
    "#CELL 4\n",
    "#PRINTS THE TOP 50 WORDS OF NEW TOKENIZED AND NORMALIZED LIST\n",
    "word_freq = nltk.FreqDist(normalized_tokens)\n",
    "top_50_words = word_freq.most_common(50)\n",
    "\n",
    "for word, freq in top_50_words:\n",
    "    print(f\"{word}: {freq}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c09ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CELL 5\n",
    "#Creating top 10 bigrams for the tokenized and normalized text\n",
    "from nltk.util import bigrams\n",
    "\n",
    "bigram_list = list(bigrams(normalized_tokens))\n",
    "\n",
    "bigram_counts = Counter(bigram_list)\n",
    "top_10_bigrams = bigram_counts.most_common(10)\n",
    "\n",
    "print(\"Top 10 Bigrams:\")\n",
    "for bigram, count in top_10_bigrams:\n",
    "    print(f\"{bigram}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63692076",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CELL 6\n",
    "#MAKING A WORD CLOUD FOR ALL TOKENS IN FULL_TEXT\n",
    "#IM NOT SURE IF WE SHOULD LEMMATIZE OR NOT, EITHER WAY S&P doesnt show up in cloud\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "all_lemmas = [lemmatizer.lemmatize(w) for w in normalized_tokens]\n",
    "\n",
    "text = \" \".join(all_lemmas)\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "word_freq = nltk.FreqDist(text)\n",
    "\n",
    "top_words = dict(word_freq.most_common(100))\n",
    "wordcloud = WordCloud(background_color='white').generate_from_frequencies(top_words)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(wordcloud, interpolation = 'bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cb0d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CELL 7\n",
    "#FILTERS THE DATASET SO ONLY FULL TEXT REPORTS \n",
    "#WHERE A SPECIFIC WORD IS MENTIONED IN IT ARE INCLUDED. IT THEN ADDS THOSE REPORTS\n",
    "#TO A NEW LIST(apple_rows) OF REPORTS THAT ALL INCLUDE THAT WORD \n",
    "#WE CAN USE THIS TO FIND BIGRAMS, DISTRIBUTIONS, and WORD CLOUDS FOR ALL REPORTS\n",
    "#THAT MENTION THAT COMPANY. \n",
    "\n",
    "import csv\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import bigrams\n",
    "from collections import Counter\n",
    "import string\n",
    "import spacy\n",
    "\n",
    "csv_file_path = 'news_stock_price.csv'\n",
    "\n",
    "column_name_to_search = 'full_text'\n",
    "\n",
    "#SPECIFIC WORD TO MENTION. We can use this to find reports that mention a specific company\n",
    "word_to_search = 'Tesla'\n",
    "\n",
    "apple_rows = []\n",
    "\n",
    "with open(csv_file_path, newline='') as csvfile:\n",
    "    myreader = csv.reader(csvfile)\n",
    "\n",
    "    next(myreader)\n",
    "    headers = next(myreader)\n",
    "\n",
    "    column_index = None\n",
    "\n",
    "    column_index = headers.index(column_name_to_search)\n",
    "    \n",
    "    if column_index is not None:\n",
    "        for row in myreader:\n",
    "            if len(row) > column_index:\n",
    "                if word_to_search in row[column_index]:\n",
    "                    apple_rows.append(row[2])\n",
    "apple_count = 0\n",
    "for text in apple_rows:\n",
    "    apple_count += text.count(word_to_search)\n",
    "print(apple_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd44a3f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'apple_rows' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 24\u001b[0m\n\u001b[1;32m     20\u001b[0m             restored_tokens\u001b[38;5;241m.\u001b[39mappend(token)\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m restored_tokens\n\u001b[0;32m---> 24\u001b[0m combined_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mapple_rows\u001b[49m)\n\u001b[1;32m     26\u001b[0m stop_words \u001b[38;5;241m=\u001b[39m stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     27\u001b[0m stop_words\u001b[38;5;241m.\u001b[39mextend([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaid\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malso\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlike\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'apple_rows' is not defined"
     ]
    }
   ],
   "source": [
    "#CELL 8\n",
    "#TOKENIZES AND NORMALIZES THE TEXT THAT MENTIONS THE COMPANY IN THE CELL ABOVE\n",
    "#PRETTY MUCH THE SAME CODE AS I TOKENIZED BEFORE\n",
    "#WE CAN ADD MORE WORDS/PHRASES TO CUSTOM TOKENIZE IF WE WANT TO PROTECT THEM (MAYBE % SIGN)\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import bigrams\n",
    "from collections import Counter\n",
    "\n",
    "def custom_tokenize(text):\n",
    "    protected_text = text.replace('S&P 500', 'SNP500')\n",
    "    tokens = nltk.word_tokenize(protected_text)\n",
    "\n",
    "    restored_tokens = []\n",
    "    for token in tokens:\n",
    "        if token == 'SNP500':\n",
    "            restored_tokens.extend(['S&P', '500'])\n",
    "        else:\n",
    "            restored_tokens.append(token)\n",
    "    \n",
    "    return restored_tokens\n",
    "\n",
    "combined_text = ' '.join(apple_rows)\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend([\"said\", \"also\", \"like\"])\n",
    "\n",
    "tokens = custom_tokenize(combined_text)\n",
    "normalized_tokens = [word for word in tokens if (word.isalpha() and word.lower() not in stop_words) or word in ['S&P', '500']]\n",
    "bigram_list = list(bigrams(normalized_tokens))\n",
    "\n",
    "bigram_counts = Counter(bigram_list)\n",
    "top_10_bigrams = bigram_counts.most_common(10)\n",
    "\n",
    "print(\"Top 10 Bigrams:\")\n",
    "for bigram, count in top_10_bigrams:\n",
    "    print(f\"{bigram}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bf782b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CELL 9\n",
    "#FINDING COLLOCATIONS\n",
    "from nltk.collocations import *\n",
    "\n",
    "def find_collocations(tokens, num):\n",
    "    bigram_measures = BigramAssocMeasures()\n",
    "    finder = BigramCollocationFinder.from_words(normalized_tokens)\n",
    "\n",
    "    #Using pmi measures with a filter of 10. \n",
    "    finder.apply_freq_filter(10)\n",
    "    top_n_bigrams = finder.nbest(bigram_measures.pmi, num)\n",
    "    bigram_freqs = [(bigram, finder.ngram_fd[bigram]) for bigram in top_n_bigrams]\n",
    "    return bigram_freqs\n",
    "apple_collocations = find_collocations(normalized_tokens, 10)\n",
    "\n",
    "print(\"Top 10 Collocations:\")\n",
    "for bigram, count in apple_collocations:\n",
    "    print(f\"{bigram}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2b6fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CEll 10\n",
    "import csv  \n",
    "from collections import defaultdict, Counter\n",
    "import pandas as pd  \n",
    "from datetime import datetime  # For handling date and time\n",
    "import matplotlib.pyplot as plt  \n",
    "\n",
    "# Defining a list of target words to track\n",
    "target_words = [\"Apple\", \"Facebook\", \"Microsoft\", \"Tesla\", \"Amazon\"]\n",
    "\n",
    "# Creating a defaultdict to store word frequencies over time\n",
    "word_frequency_over_time = defaultdict(Counter)\n",
    "\n",
    "with open('news_stock_price.csv', newline='', encoding='utf-8') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader)  # Skipping the header row\n",
    "    for row in reader:\n",
    "        # Extracting news text and date from the row\n",
    "        news_text = row[2]\n",
    "        date = datetime.strptime(row[0], '%Y-%m-%d')  # Converting date string to datetime object\n",
    "\n",
    "        # Splitting the news text into tokens (words)\n",
    "        tokens = news_text.split()\n",
    "        # Counting occurrences of target words in the news text\n",
    "        word_counts = Counter(word for word in tokens if word in target_words)\n",
    "        \n",
    "        word_frequency_over_time[date].update(word_counts)\n",
    "\n",
    "# Converting the defaultdict to a pandas DataFrame\n",
    "df = pd.DataFrame.from_dict(word_frequency_over_time, orient='index')\n",
    "# Filling missing values with 0\n",
    "df = df.fillna(0)\n",
    "\n",
    "# Plotting the data\n",
    "df.plot(figsize=(12, 6))\n",
    "plt.xlabel('Date')  \n",
    "plt.ylabel('Word Frequency')  \n",
    "plt.title('Word Frequency \n",
    "plt.legend(target_words)  \n",
    "plt.xticks(rotation=45)  \n",
    "plt.grid(True) \n",
    "\n",
    "# Saving the plot as an image file\n",
    "plt.savefig('word_frequency_trends.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7719f882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import spacy  # For recognizing entities using \"en_core_web_lg\" model from spaCy\n",
    "import csv  \n",
    "from collections import Counter  # For counting entity occurrences\n",
    "\n",
    "# Load the large English model from Spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Function to extract specified entities from text\n",
    "def extract_entities(text):\n",
    "    # Processing the text with the Spacy NLP model\n",
    "    doc = nlp(text)\n",
    "    # Returning entities that are organizations, geopolitical entities, or products\n",
    "    return [ent.text for ent in doc.ents if ent.label_ in [\"ORG\", \"GPE\", \"PRODUCT\"]]\n",
    "\n",
    "# Initializing a counter to keep track of entity occurrences\n",
    "entity_counter = Counter()\n",
    "\n",
    "# Open and read the CSV file\n",
    "with open('news_stock_price.csv', newline='', encoding='utf-8') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader)  \n",
    "    # Iterating over each row in the CSV file\n",
    "    for row in reader:\n",
    "        news_text = row[2]\n",
    "        # Extracting entities from the news text\n",
    "        entities = extract_entities(news_text)\n",
    "        # Updating the counter with these entities\n",
    "        entity_counter.update(entities)\n",
    "\n",
    "# Printing the 10 most common entities\n",
    "print(\"Most common entities:\")\n",
    "for entity, count in entity_counter.most_common(10):\n",
    "    print(f\"{entity}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc115eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#draw plot using the ouput of last block, there is only eight since we combined \"US and U.S., \"NASDAQ and Nasdaq\"\n",
    "entities = [\"U.S.\", \"Fed\", \"Nasdaq\",\"Russia\", \"NYSE\", \"Ukraine\", \"China\", \"Treasury\"]\n",
    "frequencies = [5556 + 2520, 5403, 1495+1221, 2126, 2022, 1877, 1664, 1234]\n",
    "\n",
    "plt.figure(figsize=(10, 6))  # Adjust the figure size as needed\n",
    "plt.bar(entities, frequencies, color='blue')\n",
    "plt.xlabel('Entities')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Most Common Entities Histogram (Combined U.S. and US)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88743c7b-478d-4eb4-9595-a1ce5fdb0ba1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
